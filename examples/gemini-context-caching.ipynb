{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Caching with Gemini\n",
    "\n",
    "This notebook demonstrates how to use the context caching feature of the Gemini. Context caching allows you to store large amounts of context (like documents or lengthy instructions) with the model once, and then refer to that cached context in subsequent requests saving up to 75% cost. When you cache a set of tokens, you can choose how long you want the cache to exist before the tokens are automatically deleted. This caching duration is called the time to live (TTL). In the example we are going to cache the a github repository to then more easily ask questions on it. \n",
    "\n",
    "How it works:\n",
    "1. You create a cache inckuding the content (text, files), an optional system instruction, and a time-to-live (TTL).\n",
    "2. When generating content, you reference the cache name instead of adding the context to the `contents`. The model uses the cached information alongside your new prompt.\n",
    "\n",
    "* Pricing: https://ai.google.dev/gemini-api/docs/pricing\n",
    "* Gemini API Caching Documentation: https://ai.google.dev/gemini-api/docs/caching?lang=python \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-genai gitingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already visited path: /private/var/folders/f1/3vdgcm01195b80qcp3t14n_m01b2f5/T/gitingest/3eaed527-02bf-47b3-a07f-e6f425819a57/jlowin-fastmcp/AGENTS.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from gitingest import ingest_async\n",
    "\n",
    "# create client\n",
    "client = genai.Client()\n",
    "\n",
    "model_id = \"gemini-2.5-pro\" # \"gemini-2.0-flash\"\n",
    "system_instruction = \"You are a helpful coding assistant with the FastMCP github repository available in context. If a users asks question about FastMCP or how to build an MCP server use the available information.\"\n",
    "\n",
    "# Load the Fast MCP repository and exclude tests and bloated pattern\n",
    "summary, tree, content = await ingest_async(\"https://github.com/jlowin/fastmcp\",exclude_patterns=\"*.json, *.css, *.js, uv.lock, python-version, tests/, .github/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Details:\n",
      "name: cachedContents/391i55qc71miw8dfql5q9cl0cbv6wvbs4qimfyy1\n",
      "model: models/gemini-2.5-pro\n",
      "expire_time: 2025-09-21T16:21:15+02:00\n",
      "Token Count: 692469 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a cached content object\n",
    "cache = client.caches.create(\n",
    "    model=model_id,\n",
    "    config=genai.types.CreateCachedContentConfig(\n",
    "      system_instruction=system_instruction,\n",
    "      contents=[content],\n",
    "      ttl=\"300s\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display the cache details\n",
    "print(f\"Cache Details:\\nname: {cache.name}\\nmodel: {cache.model}\\nexpire_time: {cache.expire_time.astimezone().isoformat(timespec='seconds')}\\nToken Count: {cache.usage_metadata.total_token_count} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached Tokens used: 692469\n",
      "No Cache Tokens used: 18\n",
      "Thoughts Tokens used: 906\n",
      "Output Tokens used: 1929\n"
     ]
    }
   ],
   "source": [
    "# Generate content using the cached prompt and document\n",
    "response = client.models.generate_content(\n",
    "  model=model_id,\n",
    "  contents=\"Build a simple MCP server for reading and writing local files under /tmp/mcp\",\n",
    "  config=genai.types.GenerateContentConfig(\n",
    "    cached_content=cache.name\n",
    "  ))\n",
    "\n",
    "# Print usage metadata for insights into the API call\n",
    "print(f\"Cached Tokens used: {response.usage_metadata.cached_content_token_count}\\nNo Cache Tokens used: {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count}\\nThoughts Tokens used: {response.usage_metadata.thoughts_token_count}\\nOutput Tokens used: {response.usage_metadata.candidates_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course. Here is a simple but complete MCP server built with FastMCP that allows for reading, writing, and listing files within a sandboxed `/tmp/mcp` directory.\n",
      "\n",
      "This server demonstrates several core FastMCP concepts:\n",
      "*   **Tools (`@mcp.tool`)** for actions that have side effects (like writing a file).\n",
      "*   **Resources (`@mcp.resource`)** for exposing data (like listing files).\n",
      "*   **Resource Templates** for dynamically generating resources based on parameters (like reading a specific file).\n",
      "\n",
      "### `file_server.py`\n",
      "\n",
      "```python\n",
      "import os\n",
      "from pathlib import Path\n",
      "from fastmcp import FastMCP, Context\n",
      "from fastmcp.exceptions import ToolError, ResourceError\n",
      "\n",
      "# --- Configuration ---\n",
      "# Define a safe base directory to prevent access to other parts of the filesystem.\n",
      "# All file operations will be restricted to this \"sandbox\".\n",
      "BASE_DIR = Path(\"/tmp/mcp_file_server\").resolve()\n",
      "\n",
      "# --- Server Setup ---\n",
      "# Create the FastMCP server instance.\n",
      "mcp = FastMCP(name=\"Local File Server\")\n",
      "\n",
      "# --- Security Helper ---\n",
      "def _resolve_safe_path(filename: str) -> Path:\n",
      "    \"\"\"\n",
      "    Safely resolve a filename within the BASE_DIR sandbox.\n",
      "\n",
      "    This is a critical security function to prevent directory traversal attacks\n",
      "    (e.g., users providing paths like '../../etc/passwd'). It ensures that\n",
      "    the resolved path is always inside the allowed BASE_DIR.\n",
      "    \"\"\"\n",
      "    # Create the base directory if it doesn't exist.\n",
      "    BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "    # Join the user-provided filename with the base directory.\n",
      "    # The initial '/' is stripped to prevent interpreting it as an absolute path.\n",
      "    unsafe_path = BASE_DIR.joinpath(filename.lstrip('/\\\\'))\n",
      "\n",
      "    # Resolve the path to its canonical form (e.g., processing '..').\n",
      "    resolved_path = unsafe_path.resolve()\n",
      "\n",
      "    # SECURITY CHECK: Ensure the final path is still within the BASE_DIR.\n",
      "    if BASE_DIR not in resolved_path.parents and resolved_path != BASE_DIR:\n",
      "        raise PermissionError(f\"Access denied: Path '{filename}' is outside the allowed directory.\")\n",
      "\n",
      "    return resolved_path\n",
      "\n",
      "# --- MCP Components ---\n",
      "\n",
      "@mcp.resource(\"files://list\")\n",
      "def list_files(ctx: Context) -> list[str]:\n",
      "    \"\"\"\n",
      "    Lists all files and directories within the /tmp/mcp_file_server sandbox.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Ensure the base directory exists before trying to list its contents.\n",
      "        BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
      "        # Use os.walk to recursively list files and directories\n",
      "        file_list = []\n",
      "        for root, dirs, files in os.walk(BASE_DIR):\n",
      "            relative_root = Path(root).relative_to(BASE_DIR)\n",
      "            for name in files:\n",
      "                file_list.append(str(relative_root / name))\n",
      "            for name in dirs:\n",
      "                file_list.append(str(relative_root / name) + \"/\") # Add trailing slash for dirs\n",
      "        return sorted(file_list)\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Failed to list files: {e}\")\n",
      "        raise ResourceError(f\"Could not list files: {e}\")\n",
      "\n",
      "@mcp.resource(\"files://read/{filename:path}\")\n",
      "def read_file(filename: str, ctx: Context) -> str:\n",
      "    \"\"\"\n",
      "    Reads the content of a specific file from the /tmp/mcp_file_server sandbox.\n",
      "    The {filename:path} template allows filenames that include subdirectories.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        safe_path = _resolve_safe_path(filename)\n",
      "        ctx.info(f\"Reading file from safe path: {safe_path}\")\n",
      "\n",
      "        if not safe_path.is_file():\n",
      "            raise FileNotFoundError(f\"'{filename}' is not a file or does not exist.\")\n",
      "\n",
      "        return safe_path.read_text()\n",
      "    except PermissionError as e:\n",
      "        ctx.error(f\"Access denied while reading '{filename}': {e}\")\n",
      "        raise ResourceError(f\"Access denied: {e}\")\n",
      "    except FileNotFoundError as e:\n",
      "        ctx.warning(f\"File not found: {filename}\")\n",
      "        raise ResourceError(str(e))\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Failed to read file '{filename}': {e}\")\n",
      "        raise ResourceError(f\"Could not read file: {e}\")\n",
      "\n",
      "@mcp.tool\n",
      "def write_file(filename: str, content: str, ctx: Context) -> str:\n",
      "    \"\"\"\n",
      "    Writes or overwrites a file with the given content in the /tmp/mcp_file_server sandbox.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        safe_path = _resolve_safe_path(filename)\n",
      "        ctx.info(f\"Writing to safe path: {safe_path}\")\n",
      "\n",
      "        # Ensure parent directory exists\n",
      "        safe_path.parent.mkdir(parents=True, exist_ok=True)\n",
      "\n",
      "        bytes_written = safe_path.write_text(content)\n",
      "        confirmation = f\"Successfully wrote {bytes_written} bytes to '{filename}'.\"\n",
      "        ctx.info(confirmation)\n",
      "        return confirmation\n",
      "    except PermissionError as e:\n",
      "        ctx.error(f\"Access denied while writing to '{filename}': {e}\")\n",
      "        raise ToolError(f\"Access denied: {e}\")\n",
      "    except Exception as e:\n",
      "        ctx.error(f\"Failed to write file '{filename}': {e}\")\n",
      "        raise ToolError(f\"Could not write to file: {e}\")\n",
      "\n",
      "\n",
      "# --- Server Execution ---\n",
      "if __name__ == \"__main__\":\n",
      "    # This block allows the server to be run directly from the command line.\n",
      "    # It defaults to STDIO transport, which is what clients like Claude Desktop expect.\n",
      "    # To run as a web service, you could use:\n",
      "    # mcp.run(transport=\"http\", port=8000)\n",
      "    print(\"Starting Local File MCP Server...\")\n",
      "    print(f\"File sandbox is located at: {BASE_DIR}\")\n",
      "    mcp.run()\n",
      "```\n",
      "\n",
      "### How to Run and Test\n",
      "\n",
      "1.  **Save the Code**: Save the code above as `file_server.py`.\n",
      "2.  **Run the Server**: Open your terminal and run the script.\n",
      "    ```bash\n",
      "    python file_server.py\n",
      "    ```\n",
      "    You will see a confirmation that the server is running and the location of the file sandbox.\n",
      "\n",
      "3.  **Test with the FastMCP Client** (Optional): You can create a separate client script to test your server programmatically.\n",
      "\n",
      "    **`test_client.py`**\n",
      "    ```python\n",
      "    import asyncio\n",
      "    from fastmcp import Client\n",
      "\n",
      "    async def main():\n",
      "        # Connect to the running server process via STDIO\n",
      "        async with Client(\"file_server.py\") as client:\n",
      "            print(\"--- Testing 'write_file' tool ---\")\n",
      "            write_result = await client.call_tool(\n",
      "                \"write_file\",\n",
      "                {\"filename\": \"hello.txt\", \"content\": \"Hello from FastMCP!\"}\n",
      "            )\n",
      "            print(f\"Write result: {write_result.data}\")\n",
      "\n",
      "            print(\"\\n--- Testing 'list_files' resource ---\")\n",
      "            files = await client.read_resource(\"files://list\")\n",
      "            print(f\"Files listed: {files[0].text}\")\n",
      "\n",
      "            print(\"\\n--- Testing 'read_file' resource template ---\")\n",
      "            content = await client.read_resource(\"files://read/hello.txt\")\n",
      "            print(f\"Content of 'hello.txt': {content[0].text}\")\n",
      "\n",
      "            print(\"\\n--- Testing security (directory traversal) ---\")\n",
      "            try:\n",
      "                await client.call_tool(\"write_file\", {\"filename\": \"../test.txt\", \"content\": \"danger\"})\n",
      "            except Exception as e:\n",
      "                print(f\"Safely caught traversal attempt: {e}\")\n",
      "\n",
      "    if __name__ == \"__main__\":\n",
      "        asyncio.run(main())\n",
      "    ```\n",
      "\n",
      "    Run the client in another terminal:\n",
      "    ```bash\n",
      "    python test_client.py\n",
      "    ```\n",
      "\n",
      "4.  **Connect with an LLM Client**: You can now install this server in any MCP-compatible client (like Claude Desktop or Cursor) and ask it to read, write, or list files. For example: \"Using your file tools, write 'hello world' to a file named `greeting.txt`\".\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Caching\n",
    "\n",
    "The Gemini API supports [implicit caching](https://ai.google.dev/gemini-api/docs/caching?lang=python), unlocking automatic 75% cost savings when your requests hit the cache! This means if you send a request to Gemini 2.5 models with a common prefix as one of previous requests, it’s eligible for a cache hit. The minimum input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 2581\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# create client\n",
    "client = genai.Client()\n",
    "model_id = \"gemini-2.5-pro\" # \"gemini-2.5-flash\n",
    "\n",
    "# upload big pdf file\n",
    "file_path = \"../assets/2025q1-alphabet-earnings-release.pdf\"\n",
    "pdf_file = client.files.upload(file=file_path)\n",
    "\n",
    "\n",
    "# count tokens\n",
    "tokens = client.models.count_tokens(model=model_id, contents=[pdf_file])\n",
    "print(f\"Tokens: {tokens.total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tokens: 2596\n"
     ]
    }
   ],
   "source": [
    "# Ask the model to summarize the earnings release\n",
    "instruction = \"Summarize the earnings release for the first quarter of 2025\"\n",
    "response_1 = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[pdf_file, instruction],\n",
    ")\n",
    "\n",
    "# Print usage metadata for insights into the API call\n",
    "print(f\"input tokens: {response_1.usage_metadata.prompt_token_count }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached tokens: 2160\n"
     ]
    }
   ],
   "source": [
    "# 2nd request which uses the cached prefix (pdf file)\n",
    "instruction = \"What are focus areas for the second quarter of 2025?\"\n",
    "response_2 = client.models.generate_content(\n",
    "    model=model_id,\n",
    "    contents=[pdf_file, instruction],\n",
    ")\n",
    "\n",
    "print(f\"cached tokens: {response_2.usage_metadata.cached_content_token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n",
      "cached\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# create client\n",
    "client = genai.Client()\n",
    "model_id = \"gemini-2.5-flash\" # \"gemini-2.5-pro\"\n",
    "\n",
    "\n",
    "cache_count = 0\n",
    "for i in range(10):\n",
    "    # upload big pdf file\n",
    "    file_path = \"../assets/2025q1-alphabet-earnings-release.pdf\"\n",
    "    pdf_file = client.files.upload(file=file_path)\n",
    "    # Ask the model to summarize the earnings release\n",
    "    instruction = \"Summarize the earnings release for the first quarter of 2025\"\n",
    "    response_1 = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[pdf_file, instruction],\n",
    "    )\n",
    "    # 2nd request which uses the cached prefix (pdf file)\n",
    "    instruction = \"What are focus areas for the second quarter of 2025?\"\n",
    "    response_2 = client.models.generate_content(\n",
    "        model=model_id,\n",
    "        contents=[pdf_file, instruction],\n",
    "    )\n",
    "    if response_2.usage_metadata.cached_content_token_count:\n",
    "        print(\"cached\")\n",
    "        cache_count += 1\n",
    "    else:\n",
    "        print(\"no cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
