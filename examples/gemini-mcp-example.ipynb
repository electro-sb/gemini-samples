{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini 2.5 with MCP (Model Context Protocol) Servers\n",
    "\n",
    "Gemini models can be used with MCP server using its native tool calling capabilities. MCP, or Model Context Protocol, is an open standard introduced by Anthropic designed to standardize how AI models like Gemini interact with external tools and data sources. Instead of requiring custom integrations for each tool, MCP provides a structured way for models to access context, such as functions (tools), data sources (resources), or pre-defined prompts. This allows AI agents to securely and efficiently connect with real-world systems and workflows.\n",
    "\n",
    "MCP server expose their tools via JSON schema definitions, which can be converted to Gemini compatible OpenAPI schema definitions. This allows you to easily use MCP server with Gemini models, below you will example on how to implement this. \n",
    "\n",
    "You can learn more about Google Search integration with Gemini here:\n",
    "- [https://ai.google.dev/gemini-api/docs/function-calling?lang=python](https://ai.google.dev/gemini-api/docs/function-calling?lang=python&example=weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Google GenAI and MCP\n",
    "%pip install google-genai mcp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example on how to use MCP with Gemini's tool calling\n",
    "\n",
    "MCPs can be used with Google DeepMind Gemini by converting the MCP tools into Gemini compatible tools. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"@openbnb/mcp-server-airbnb\",\n",
    "        \"--ignore-robots-txt\",\n",
    "    ],  # Optional command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(\n",
    "            read,\n",
    "            write,\n",
    "        ) as session:\n",
    "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get tools from MCP session and convert to Gemini Tool objects\n",
    "            mcp_tools = await session.list_tools()\n",
    "            tools = types.Tool(function_declarations=[\n",
    "                {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description,\n",
    "                    \"parameters\": tool.inputSchema,\n",
    "                }\n",
    "                for tool in mcp_tools.tools\n",
    "            ])\n",
    "            \n",
    "            # Send request with function declarations\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash\",  # Or your preferred model supporting function calling\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=0.7,\n",
    "                    tools=[tools],\n",
    "                ),  # Example other config\n",
    "            )\n",
    "        # Check for a function call\n",
    "        if response.candidates[0].content.parts[0].function_call:\n",
    "            function_call = response.candidates[0].content.parts[0].function_call\n",
    "            print(f\"Function to call: {function_call.name}\")\n",
    "            print(f\"Arguments: {function_call.args}\")\n",
    "            # In a real app, you would call your function here:\n",
    "            # result = await session.call_tool(function_call.args, arguments=function_call.args)\n",
    "            # sent new request with function call\n",
    "        else:\n",
    "            print(\"No function call found in the response.\")\n",
    "            print(response.text)\n",
    "            \n",
    "await run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Agentic example with Gemini and Airbnb MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "import os\n",
    "\n",
    "client = genai.Client()\n",
    "model = \"gemini-2.5-flash\"\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"npx\",  # Executable\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"@openbnb/mcp-server-airbnb\",\n",
    "        \"--ignore-robots-txt\",\n",
    "    ],  # Optional command line arguments\n",
    "    env=None,  # Optional environment variables\n",
    ")\n",
    "\n",
    "async def agent_loop(prompt: str, client: genai.Client, session: ClientSession):\n",
    "    contents = [types.Content(role=\"user\", parts=[types.Part(text=prompt)])]\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    \n",
    "    # --- 1. Get Tools from Session and convert to Gemini Tool objects ---\n",
    "    mcp_tools = await session.list_tools()\n",
    "    tools = types.Tool(function_declarations=[\n",
    "        {\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"parameters\": tool.inputSchema,\n",
    "        }\n",
    "        for tool in mcp_tools.tools\n",
    "    ])\n",
    "    \n",
    "    # --- 2. Initial Request with user prompt and function declarations ---\n",
    "    response = await client.aio.models.generate_content(\n",
    "        model=model,  # Or your preferred model supporting function calling\n",
    "        contents=contents,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            tools=[tools],\n",
    "        ),  # Example other config\n",
    "    )\n",
    "    \n",
    "    # --- 3. Append initial response to contents ---\n",
    "    contents.append(response.candidates[0].content)\n",
    "\n",
    "    # --- 4. Tool Calling Loop ---            \n",
    "    turn_count = 0\n",
    "    max_tool_turns = 5\n",
    "    while response.function_calls and turn_count < max_tool_turns:\n",
    "        turn_count += 1\n",
    "        tool_response_parts: List[types.Part] = []\n",
    "\n",
    "        # --- 4.1 Process all function calls in order and return in this turn ---\n",
    "        for fc_part in response.function_calls:\n",
    "            tool_name = fc_part.name\n",
    "            args = fc_part.args or {}  # Ensure args is a dict\n",
    "            print(f\"Attempting to call MCP tool: '{tool_name}' with args: {args}\")\n",
    "\n",
    "            tool_response: dict\n",
    "            try:\n",
    "                # Call the session's tool executor\n",
    "                tool_result = await session.call_tool(tool_name, args)\n",
    "                print(f\"MCP tool '{tool_name}' executed successfully.\")\n",
    "                if tool_result.isError:\n",
    "                    tool_response = {\"error\": tool_result.content[0].text}\n",
    "                else:\n",
    "                    tool_response = {\"result\": tool_result.content[0].text}\n",
    "            except Exception as e:\n",
    "                tool_response = {\"error\":  f\"Tool execution failed: {type(e).__name__}: {e}\"}\n",
    "            \n",
    "            # Prepare FunctionResponse Part\n",
    "            tool_response_parts.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=tool_name, response=tool_response\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # --- 4.2 Add the tool response(s) to history ---\n",
    "        contents.append(types.Content(role=\"user\", parts=tool_response_parts))\n",
    "        print(f\"Added {len(tool_response_parts)} tool response parts to history.\")\n",
    "\n",
    "        # --- 4.3 Make the next call to the model with updated history ---\n",
    "        print(\"Making subsequent API call with tool responses...\")\n",
    "        response = await client.aio.models.generate_content(\n",
    "            model=model,\n",
    "            contents=contents,  # Send updated history\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=1.0,\n",
    "                tools=[tools],\n",
    "            ),  # Keep sending same config\n",
    "        )\n",
    "        contents.append(response.candidates[0].content)\n",
    "\n",
    "    if turn_count >= max_tool_turns and response.function_calls:\n",
    "        print(f\"Maximum tool turns ({max_tool_turns}) reached. Exiting loop.\")\n",
    "\n",
    "    print(\"MCP tool calling loop finished. Returning final response.\")\n",
    "    # --- 5. Return Final Response ---\n",
    "    return response\n",
    "        \n",
    "async def run():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(\n",
    "            read,\n",
    "            write,\n",
    "        ) as session:\n",
    "            # Test prompt\n",
    "            prompt = \"I want to book an apartment in Paris for 2 nights. 03/28 - 03/30\"\n",
    "            print(f\"Running agent loop with prompt: {prompt}\")\n",
    "            # Run agent loop\n",
    "            res = await agent_loop(prompt, client, session)\n",
    "            return res\n",
    "res = await run()\n",
    "print(res.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
